{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Logistic Regression with parameter search\n",
    "\n",
    "This code demonstrates how to do a grid search for a regularization parameter C using multiclass logistic regression on the famous Iris dataset.\n",
    "\n",
    "Two different search methods are used:\n",
    "\n",
    "- LogisticRegressionCV\n",
    "- GridSearchCV\n",
    "\n",
    "**This code uses scikit learn 0.18. scikit learn changed packages from 0.17 to 0.18. You might need to modify the import calls if using 0.17. Upgrading to 0.18 is recommended.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the Iris Dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_features = iris.data\n",
    "Y_targets = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hold out a test set to test real-world performance\n",
    "# use a stratified test split for demonstration\n",
    "# (although unstratified will likely be a better simulation of real-world behavior)\n",
    "# stratified is recommended when dealing with rare classes\n",
    "# do 10 splits to keep .10 of the data as the test set\n",
    "kfold_generator = StratifiedKFold(n_splits = 10, random_state=42)\n",
    "train_indices, test_indices = kfold_generator.split(X_features, Y_targets).next()\n",
    "\n",
    "X_train = X_features[train_indices, :]\n",
    "Y_train = Y_targets[train_indices]\n",
    "\n",
    "X_test = X_features[test_indices, :]\n",
    "Y_test = Y_targets[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((15, 4), (15,))\n"
     ]
    }
   ],
   "source": [
    "# print out the test set for demo purposes\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris type 0, count: 45\n",
      "iris type 1, count: 45\n",
      "iris type 2, count: 45\n"
     ]
    }
   ],
   "source": [
    "# count the iris values in the dataset\n",
    "iris_values, counts = np.unique(Y_train, return_counts=True)\n",
    "for value, count in zip(iris_values, counts):\n",
    "    print(\"iris type %i, count: %i\" % (value, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-03   1.00000000e-02   1.00000000e-01   1.00000000e+00\n",
      "   1.00000000e+01   1.00000000e+02   1.00000000e+03   1.00000000e+04]\n"
     ]
    }
   ],
   "source": [
    "# Set a search space for the C regularization parameter\n",
    "# C is the inverse of lambda. A smaller C is larger regularization.\n",
    "\n",
    "minCExp = -3\n",
    "maxCExp = 4\n",
    "Cs = np.logspace(minCExp, maxCExp, num = ( maxCExp - minCExp + 1))\n",
    "print(Cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using Logistic Regresion CV\n",
    "\n",
    "My testing below suggests that the scoring functions for LogisticRegressionCV may be having a strange behavior that results in scoring that is different from GridSearchCV or from a manual cross-validation search. I recommend using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_method = StratifiedKFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we set the Cs to be the search space of the C regularization parameter we defined above.\n",
    "# we set cv to 5 to do 5 different folds of cross validation\n",
    "# we set the scoring function to be log_loss, which is the cross-entropy loss\n",
    "# we set the multi_class to be multinomial\n",
    "\n",
    "logreg_cv = LogisticRegressionCV(Cs = Cs, cv=cv_method, \n",
    "                                 scoring='neg_log_loss', multi_class='multinomial', \n",
    "                                 solver='lbfgs', max_iter=10000, tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([  1.00000e-03,   1.00000e-02,   1.00000e-01,   1.00000e+00,\n",
       "         1.00000e+01,   1.00000e+02,   1.00000e+03,   1.00000e+04]),\n",
       "           class_weight=None,\n",
       "           cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=False),\n",
       "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
       "           max_iter=10000, multi_class='multinomial', n_jobs=1,\n",
       "           penalty='l2', random_state=None, refit=True,\n",
       "           scoring='neg_log_loss', solver='lbfgs', tol=1e-06, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the data using CV.\n",
    "# note that we're not using a test set, which is typically a good thing to do\n",
    "logreg_cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For multinomial prediction, a single C value will be chosen and shared across classes. For One Versus Rest prediction, multiple C values may be chosen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1000.,  1000.,  1000.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_cv.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, -1.0402278943216992),\n",
       " (0.01, -0.84784093119018067),\n",
       " (0.10000000000000001, -0.65029934097579412),\n",
       " (1.0, -0.61229837068843496),\n",
       " (10.0, -0.58909076888789169),\n",
       " (100.0, -0.56296608339524501),\n",
       " (1000.0, -0.54538213334197894),\n",
       " (10000.0, -0.54740814400327786)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average the scores across folds\n",
    "zip(Cs, np.mean(logreg_cv.scores_[0], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that it appears that scikit learn takes the negative of the log loss (which is in turn the negative of the log likelihoods) in the scoring function used by the cross validation packages.*\n",
    "\n",
    "*Also, the loss from the LogisticRegresionCV seems to be returning some kind of unnormalized scores (maybe the raw score, not the mean negative log loss). It raises warning flags IMO.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044204117903223913"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set performance\n",
    "log_loss(Y_train, logreg_cv.predict_proba(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00035985093100667984"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set performance\n",
    "log_loss(Y_test, logreg_cv.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053977639651001975"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set performance\n",
    "10 * Y_test.shape[0] * log_loss(Y_test, logreg_cv.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a cross validation method\n",
    "# Here stratified k fold is used. Stratified K fold may perform better for rare targets.\n",
    "# but in reality, we will not have evenly matched training and test sets, so unstratified sampling may provide\n",
    "# more realistic estimates\n",
    "cv_method = StratifiedKFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(random_state=42, max_iter = 10000, tol=1e-6, \n",
    "                                                 multi_class='multinomial', solver='lbfgs')\n",
    "param_grid = {'C': Cs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = sklearn.model_selection.GridSearchCV(logreg, param_grid=param_grid, cv = cv_method, scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=42, solver='lbfgs',\n",
       "          tol=1e-06, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([  1.00000e-03,   1.00000e-02,   1.00000e-01,   1.00000e+00,\n",
       "         1.00000e+01,   1.00000e+02,   1.00000e+03,   1.00000e+04])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.00445914,  0.00763273,  0.01266565,  0.01918077,  0.0285008 ,\n",
       "         0.03825641,  0.05675402,  0.04506173]),\n",
       " 'mean_score_time': array([ 0.00073848,  0.00075407,  0.00066075,  0.00069647,  0.00066314,\n",
       "         0.00065975,  0.00071096,  0.00064626]),\n",
       " 'mean_test_score': array([-0.98417471, -0.65474451, -0.33890824, -0.14691963, -0.07560623,\n",
       "        -0.0640402 , -0.08313179, -0.11551374]),\n",
       " 'mean_train_score': array([-0.98393976, -0.65384028, -0.33617377, -0.14047801, -0.06600018,\n",
       "        -0.04540575, -0.04039159, -0.03944168]),\n",
       " 'param_C': masked_array(data = [0.001 0.01 0.10000000000000001 1.0 10.0 100.0 1000.0 10000.0],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.10000000000000001},\n",
       "  {'C': 1.0},\n",
       "  {'C': 10.0},\n",
       "  {'C': 100.0},\n",
       "  {'C': 1000.0},\n",
       "  {'C': 10000.0}),\n",
       " 'rank_test_score': array([8, 7, 6, 5, 2, 1, 3, 4], dtype=int32),\n",
       " 'split0_test_score': array([-0.98281161, -0.65219284, -0.34322115, -0.14682101, -0.05016135,\n",
       "        -0.02021636, -0.01838045, -0.01895004]),\n",
       " 'split0_train_score': array([-0.9845614 , -0.65393343, -0.33420247, -0.14496261, -0.07567908,\n",
       "        -0.05656121, -0.05324328, -0.05304796]),\n",
       " 'split1_test_score': array([-0.98054442, -0.64261724, -0.32366405, -0.14324787, -0.09226877,\n",
       "        -0.08269426, -0.08439962, -0.08496535]),\n",
       " 'split1_train_score': array([-0.98593601, -0.65773403, -0.33897371, -0.13845912, -0.06191625,\n",
       "        -0.04448173, -0.04251349, -0.042395  ]),\n",
       " 'split2_test_score': array([-0.98583455, -0.66229304, -0.35479354, -0.16437818, -0.08616151,\n",
       "        -0.0578027 , -0.04685094, -0.04473245]),\n",
       " 'split2_train_score': array([-0.98300406, -0.65083664, -0.33128108, -0.13584381, -0.06428308,\n",
       "        -0.0480327 , -0.04610103, -0.0459818 ]),\n",
       " 'split3_test_score': array([-0.9831216 , -0.65000516, -0.33981747, -0.16606722, -0.12082955,\n",
       "        -0.15163688, -0.2631242 , -0.42764934]),\n",
       " 'split3_train_score': array([ -9.84557050e-01,  -6.55003110e-01,  -3.34216269e-01,\n",
       "         -1.32146735e-01,  -5.04321297e-02,  -1.99635665e-02,\n",
       "         -5.12066949e-03,  -9.62609133e-04]),\n",
       " 'split4_test_score': array([-0.98856138, -0.6666143 , -0.33304499, -0.11408388, -0.02860998,\n",
       "        -0.00785082, -0.00290371, -0.00127153]),\n",
       " 'split4_train_score': array([-0.98164027, -0.65169418, -0.34219533, -0.15097779, -0.07769037,\n",
       "        -0.05798954, -0.05497948, -0.05482102]),\n",
       " 'std_fit_time': array([ 0.00022313,  0.00030677,  0.00097467,  0.00333278,  0.00429904,\n",
       "         0.00697548,  0.00855197,  0.01586332]),\n",
       " 'std_score_time': array([  4.79017518e-05,   8.49789372e-05,   2.98568608e-05,\n",
       "          7.20098946e-05,   3.47164305e-05,   3.69585405e-05,\n",
       "          5.32778844e-05,   1.65446690e-05]),\n",
       " 'std_test_score': array([ 0.00276203,  0.00864772,  0.01037819,  0.01877823,  0.03252781,\n",
       "         0.0512598 ,  0.09417479,  0.15858604]),\n",
       " 'std_train_score': array([ 0.00147754,  0.00245559,  0.00389243,  0.00671295,  0.00992303,\n",
       "         0.01369566,  0.01821598,  0.01976759])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that it appears that scikit learn takes the negative of the log loss (which is in turn the negative of the log likelihoods) in the scoring function used by the cross validation packages. Also, they return the unnormalized scores (i.e. the raw score, not the mean negative log loss).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, -0.98417470988523692),\n",
       " (0.01, -0.65474451399536815),\n",
       " (0.10000000000000001, -0.33890824155594385),\n",
       " (1.0, -0.14691963048684026),\n",
       " (10.0, -0.075606233996511577),\n",
       " (100.0, -0.064040204703850009),\n",
       " (1000.0, -0.083131785705103461),\n",
       " (10000.0, -0.11551374162484519)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(Cs, gs.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=42, solver='lbfgs',\n",
      "          tol=1e-06, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "best_model = gs.best_estimator_\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04681072157696399"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set performance\n",
    "log_loss(Y_train, best_model.predict_proba(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017725933203487803"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set performance\n",
    "log_loss(Y_test, best_model.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Logistic Regression\n",
    "\n",
    "It isn't totally clear why the LogisticRegressionCV and GridSearchCV arrive at different results. They have slightly different best C parameters and log loss estimates.\n",
    "\n",
    "It appears that there is simply a different randomization used for the crossvalidation fold generation in the two examples. You can see that both models use the same base model based on the replication of the results below.\n",
    "\n",
    "In general, I would recommend using the GridSearchCV function because of it's reusability across multiple model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegresionCV best C value: 1000.000000\n",
      "training set performance: 0.044206\n",
      "test set performance: 0.000364\n"
     ]
    }
   ],
   "source": [
    "best_C = logreg_cv.C_[0]\n",
    "print(\"LogisticRegresionCV best C value: %f\" % best_C)\n",
    "logreg_model = sklearn.linear_model.LogisticRegression(random_state=42, max_iter = 10000, tol=1e-6, \n",
    "                                                 multi_class='multinomial', solver='lbfgs', C = logreg_cv.C_[0])\n",
    "logreg_model.fit(X_train, Y_train)\n",
    "\n",
    "# training set performance\n",
    "print(\"training set performance: %f\" % log_loss(Y_train, logreg_model.predict_proba(X_train)))\n",
    "\n",
    "# test set performance\n",
    "print(\"test set performance: %f\" % log_loss(Y_test, logreg_model.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV best C value: 100.000000\n",
      "training set performance: 0.046811\n",
      "test set performance: 0.001773\n"
     ]
    }
   ],
   "source": [
    "best_C = gs.best_params_['C']\n",
    "print(\"GridSearchCV best C value: %f\" % best_C)\n",
    "logreg_model = sklearn.linear_model.LogisticRegression(random_state=42, max_iter = 10000, tol=1e-6, \n",
    "                                                 multi_class='multinomial', solver='lbfgs', C = gs.best_params_['C'])\n",
    "logreg_model.fit(X_train, Y_train)\n",
    "\n",
    "# training set performance\n",
    "print(\"training set performance: %f\" % log_loss(Y_train, logreg_model.predict_proba(X_train)))\n",
    "\n",
    "# test set performance\n",
    "print(\"test set performance: %f\" % log_loss(Y_test, logreg_model.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using Logistic Regression with Manual Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_method = StratifiedKFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_scores = np.zeros(5)\n",
    "for i, (train, cv) in enumerate(cv_method.split(X_train, Y_train)):\n",
    "    logreg_model = logreg = sklearn.linear_model.LogisticRegression(random_state=42, max_iter = 10000, tol=1e-6, \n",
    "                                                 multi_class='multinomial', solver='lbfgs',\n",
    "                                                                   C = 1000.0)\n",
    "    logreg_model.fit(X_train[train, :], Y_train[train])\n",
    "    logreg_model.predict(X_train[cv, :])\n",
    "    cv_scores[i] = log_loss(Y_train[cv], logreg_model.predict_proba(X_train[cv, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.083131785705103448"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
